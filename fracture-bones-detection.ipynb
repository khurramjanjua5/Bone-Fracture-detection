{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4507366,"sourceType":"datasetVersion","datasetId":2634341}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten, Dense\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T21:08:20.995689Z","iopub.execute_input":"2024-03-26T21:08:20.996270Z","iopub.status.idle":"2024-03-26T21:08:21.002211Z","shell.execute_reply.started":"2024-03-26T21:08:20.996241Z","shell.execute_reply":"2024-03-26T21:08:21.001248Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nimport math\n\n# Define a more aggressive data augmentation for the training data\ntrain_datagenerator = ImageDataGenerator(\n    rescale=1.0/255,\n    shear_range=0.2,\n    zoom_range=0.5,\n    horizontal_flip=True,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    brightness_range=[0.5, 1.5],\n    fill_mode='nearest'\n)\n\ntest_datagenerator = ImageDataGenerator(rescale = 1.0/255)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:08:41.210001Z","iopub.execute_input":"2024-03-26T21:08:41.210690Z","iopub.status.idle":"2024-03-26T21:08:41.219802Z","shell.execute_reply.started":"2024-03-26T21:08:41.210650Z","shell.execute_reply":"2024-03-26T21:08:41.218965Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data = train_datagenerator.flow_from_directory('/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/train',\n                                                    target_size=(224, 224),\n                                                    batch_size = 50,\n                                                    class_mode = 'categorical')\nval_data = test_datagenerator.flow_from_directory('/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/val',\n                                                  target_size=(224, 224),\n                                                    batch_size = 50,\n                                                    class_mode = 'categorical')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T21:15:38.722975Z","iopub.execute_input":"2024-03-26T21:15:38.723904Z","iopub.status.idle":"2024-03-26T21:15:42.171594Z","shell.execute_reply.started":"2024-03-26T21:15:38.723872Z","shell.execute_reply":"2024-03-26T21:15:42.170694Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Found 8863 images belonging to 2 classes.\nFound 600 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained_model = tf.keras.applications.DenseNet121(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:04:36.470448Z","iopub.execute_input":"2024-03-26T22:04:36.471318Z","iopub.status.idle":"2024-03-26T22:04:39.167149Z","shell.execute_reply.started":"2024-03-26T22:04:36.471281Z","shell.execute_reply":"2024-03-26T22:04:39.166328Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"def lr_schedule(epoch):\n    initial_lr = 0.001\n    decay_factor = 0.9\n    return initial_lr * math.pow(decay_factor, epoch)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:04:43.622415Z","iopub.execute_input":"2024-03-26T22:04:43.623035Z","iopub.status.idle":"2024-03-26T22:04:43.627525Z","shell.execute_reply.started":"2024-03-26T22:04:43.623002Z","shell.execute_reply":"2024-03-26T22:04:43.626688Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"for layer in pretrained_model.layers[:-100]: #fine tune last 100 layers\n    layer.trainable = True\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:04:47.478925Z","iopub.execute_input":"2024-03-26T22:04:47.479268Z","iopub.status.idle":"2024-03-26T22:04:47.486132Z","shell.execute_reply.started":"2024-03-26T22:04:47.479242Z","shell.execute_reply":"2024-03-26T22:04:47.485252Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:06:50.255330Z","iopub.execute_input":"2024-03-26T22:06:50.256065Z","iopub.status.idle":"2024-03-26T22:06:50.260259Z","shell.execute_reply.started":"2024-03-26T22:06:50.256033Z","shell.execute_reply":"2024-03-26T22:06:50.259371Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    pretrained_model,\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(2, activation='softmax')\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:05:53.994460Z","iopub.execute_input":"2024-03-26T22:05:53.995081Z","iopub.status.idle":"2024-03-26T22:05:54.005664Z","shell.execute_reply.started":"2024-03-26T22:05:53.995048Z","shell.execute_reply":"2024-03-26T22:05:54.004682Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:05:57.799345Z","iopub.execute_input":"2024-03-26T22:05:57.800211Z","iopub.status.idle":"2024-03-26T22:05:57.809564Z","shell.execute_reply.started":"2024-03-26T22:05:57.800178Z","shell.execute_reply":"2024-03-26T22:05:57.808677Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    epochs=10,\n    validation_data=val_data,\n    callbacks=[early_stopping, LearningRateScheduler(lr_schedule)]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:06:56.513787Z","iopub.execute_input":"2024-03-26T22:06:56.514527Z","iopub.status.idle":"2024-03-26T22:25:55.515245Z","shell.execute_reply.started":"2024-03-26T22:06:56.514495Z","shell.execute_reply":"2024-03-26T22:25:55.514325Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711491003.613428     103 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 13/178\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:48\u001b[0m 9s/step - accuracy: 0.5735 - loss: 0.9346  ","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711491107.771989     101 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5410 - loss: 0.7997","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1711491229.349897     100 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - accuracy: 0.5411 - loss: 0.7993 - val_accuracy: 0.5717 - val_loss: 0.6468 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 652ms/step - accuracy: 0.6150 - loss: 0.6422 - val_accuracy: 0.6550 - val_loss: 0.5611 - learning_rate: 9.0000e-04\nEpoch 3/10\n\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 651ms/step - accuracy: 0.6804 - loss: 0.5839 - val_accuracy: 0.6233 - val_loss: 0.6160 - learning_rate: 8.1000e-04\nEpoch 4/10\n\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 648ms/step - accuracy: 0.7408 - loss: 0.5144 - val_accuracy: 0.6150 - val_loss: 1.4656 - learning_rate: 7.2900e-04\nEpoch 5/10\n\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 642ms/step - accuracy: 0.8044 - loss: 0.4194 - val_accuracy: 0.6217 - val_loss: 1.2055 - learning_rate: 6.5610e-04\nEpoch 6/10\n\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 651ms/step - accuracy: 0.8561 - loss: 0.3262 - val_accuracy: 0.6700 - val_loss: 0.8894 - learning_rate: 5.9049e-04\nEpoch 7/10\n\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 655ms/step - accuracy: 0.9053 - loss: 0.2298 - val_accuracy: 0.7500 - val_loss: 1.5941 - learning_rate: 5.3144e-04\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimage_path = '/kaggle/input/bone-fracture-detection-using-xrays/archive (6)/val/fractured/1-rotated1-rotated3-rotated3.jpg'\nimage = Image.open(image_path)\nimage = image.resize((224, 224))  # Resize to match the input shape of the model\nimage = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\nimage = np.expand_dims(image, axis=0)  # Add batch dimension\n\n# Perform prediction\npredictions = model.predict(image)\n\n# Interpret predictions\npredicted_class = np.argmax(predictions)  # Get the index of the class with highest probability\nclass_names = ['fractured', 'normal']  # Define class names\npredicted_class_name = class_names[predicted_class]\n\nprint(\"Predicted class:\", predicted_class_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T22:29:41.872804Z","iopub.execute_input":"2024-03-26T22:29:41.873432Z","iopub.status.idle":"2024-03-26T22:29:41.959696Z","shell.execute_reply.started":"2024-03-26T22:29:41.873394Z","shell.execute_reply":"2024-03-26T22:29:41.958901Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\nPredicted class: fractured\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}